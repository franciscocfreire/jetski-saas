# Alertmanager Configuration for Jetski SaaS
# Handles alert routing and notifications

global:
  resolve_timeout: 5m
  # Slack webhook (configure in production)
  # slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Templates for alert notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  # Default receiver for all alerts
  receiver: 'team-alerts'

  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'service']

  # Wait before sending first notification
  group_wait: 10s

  # Wait before sending notification about new alerts in same group
  group_interval: 10s

  # Wait before re-sending notification
  repeat_interval: 12h

  # Child routes with specific matchers
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m
      continue: true

    # Warning alerts - less urgent
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      repeat_interval: 3h

    # Database alerts
    - match:
        component: database
      receiver: 'database-team'
      group_by: ['alertname', 'tenant_id']

    # Backend API alerts
    - match:
        component: backend
      receiver: 'backend-team'

# Alert receivers/integrations
receivers:
  # Default team alerts (email + webhook)
  - name: 'team-alerts'
    email_configs:
      - to: 'team@jetski.com'
        from: 'alertmanager@jetski.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@jetski.com'
        auth_password: 'APP_PASSWORD_HERE'
        headers:
          Subject: '[Jetski] {{ .GroupLabels.alertname }}'

    webhook_configs:
      - url: 'http://localhost:5001/alerts'
        send_resolved: true

  # Critical alerts (Slack + PagerDuty)
  - name: 'critical-alerts'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'Alertmanager'
        color: 'danger'
        title: '🚨 CRITICAL: {{ .GroupLabels.alertname }}'
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Severity:* {{ .Labels.severity }}
            *Component:* {{ .Labels.component }}
          {{ end }}
        send_resolved: true

    # Uncomment for PagerDuty integration
    # pagerduty_configs:
    #   - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
    #     description: '{{ .GroupLabels.alertname }}'

  # Warning alerts (Slack only)
  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        username: 'Alertmanager'
        color: 'warning'
        title: '⚠️ WARNING: {{ .GroupLabels.alertname }}'
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Component:* {{ .Labels.component }}
          {{ end }}
        send_resolved: true

  # Database team
  - name: 'database-team'
    email_configs:
      - to: 'database-team@jetski.com'
        from: 'alertmanager@jetski.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@jetski.com'
        auth_password: 'APP_PASSWORD_HERE'
        headers:
          Subject: '[DB Alert] {{ .GroupLabels.alertname }}'

  # Backend team
  - name: 'backend-team'
    email_configs:
      - to: 'backend-team@jetski.com'
        from: 'alertmanager@jetski.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@jetski.com'
        auth_password: 'APP_PASSWORD_HERE'
        headers:
          Subject: '[Backend Alert] {{ .GroupLabels.alertname }}'

# Inhibition rules - suppress alerts
inhibit_rules:
  # Suppress warning if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']

  # Suppress all alerts if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']
